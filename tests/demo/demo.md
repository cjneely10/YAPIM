# Simple tutorial for creating a data workflow

The following demo will provide an introduction to the features available within the YAPIM API. We will be developing a very simple pipeline to find proteins in bacterial genomes/metagenome-assembled-genomes and to provide functional annotation of genomes that pass a quality check step. This three-part tutorial is succeeded by a larger 7-part tutorial for more intermediate users who may wish to learn about the more complex features available.

## Your first YAPIM workflow

Let's plan our data pipeline and consider tools we can use to accomplish these steps.

1. Identify proteins in the input genome
    1. This can be completed using `prodigal`:
        1. `conda install -c bioconda prodigal`
2. Determine the quality of each assembly
    1. `checkm` provides this feature:
        1. See [the installation wiki](https://github.com/Ecogenomics/CheckM/wiki/Installation) for dependency download information
        2. `pip install numpy matplotlib pysam`
        3. `pip install checkm-genome`
3. Annotate proteins with PFam
    1. We will use `mmseqs` here:
        1. `conda install -c bioconda mmseqs2`


## Step 1: Identify proteins in each input genome

The primary logic for creating a running a YAPIM pipeline will be enclosed in classes we write that inherit from `Task` or `AggregateTask`.  

 

We begin be creating a new file named `identify_proteins.py` inside of our “tasks” directory.  In this file we will create our first class, which we will name `IdentifyProteins`, and have it inherit from `Task`. Because `Task` is an abstract class, we must provide definitions for any abstract methods it contains.

```python
from typing import List, Union, Type

from yapim import Task, DependencyInput


class IdentifyProteins(Task):
    @staticmethod
    def requires() -> List[Union[str, Type]]:
        pass

    @staticmethod
    def depends() -> List[DependencyInput]:
        pass

    def run(self):
        pass
```

Note that type annotations are not directly required for Python, but they are a helpful sanity check, particularly for users running IDEs like PyCharm. 

As alluded to in the `Task` Lifecycle and Methods section, we need to define any tasks that `IdentifyProteins` needs completed to run, and we need to define how `IdentifyProteins` will run itself, along with any output it might generate. 

Since we want this to be the first step in the pipeline, this task will require no other task to be completed before it, so we leave the definition of the `requires()` method as is. We can also ignore the `depends()` method for now, as this will be covered in a subsequent set of tutorials.

#### Calling a program from within YAPIM:

YAPIM makes heavy use of the python library plumbum, which provides an incredibly useful API to run CLI-like commands from within python code. A typical CLI invocation using plumbum resembles: 

 

```python
local["echo"]["Hello", "world!"]()
```

 

The above command runs the unix program `echo` from the local system with the provided arguments. From within our class, we can invoke the program assigned to this task by using:

```python
self.local["echo"]["Hello", "world!"]()
self.program["Hello", "world!"]()  # Alias for above command if set in configuration file
```

#### Using helper methods and attributes in `Task`

Let’s use this opportunity to explore a bit more of the API. YAPIM provides several helper attributes and methods that allow us to simplify and automate much of the calling code: 

 

```
.wdir => the working directory in which this task is running 

.threads => number of threads available to launch the program. 

.input => a dictionary of all input that this task was launched with. See the InputLoader section for more information. 

.output => a dictionary defining all of the output that we expect to be generated by this program after the run() method completes. 

.added_flags() => extra flags that are to be passed to the program. 

.flags_to_list(name) => return a configuration file section as a list. 

.parallel(cmd) => parallelize a command across potentially multiple compute nodes.

.single(cmd) => parallelize a command across potentially multiple compute nodes and allot only one thread per call.
```
 

For a complete list of the methods and attributes available to YAPIM tasks, see the provided documentation.

#### Finishing our implementation

Let us define the command that will call `prodigal`:

```python
from typing import List, Union, Type

from yapim import Task, DependencyInput


class IdentifyProteins(Task):
    @staticmethod
    def requires() -> List[Union[str, Type]]:
        pass

    @staticmethod
    def depends() -> List[DependencyInput]:
        pass

    def run(self):
        self.single(
            self.program[
                "-i", self.input["fasta"],
                "-a", self.wdir.joinpath("proteins.faa"),
                (*self.added_flags)
            ]
        )
```

From the `prodigal` documentation, we can output proteins in FASTA format using the `-a` flag. We pass as input the FASTA file that was provided at runtime, and we set the `-a` flag to output our results to the file names `proteins.faa`, which is stored in the working directory for this `Task`.

This is great, but so far all we have done is run `prodigal`. Ideally, we want output from this step to be available to downstream analysis steps. To do this, we can define the expected output of this task within its initializer:

```python
from typing import List, Union, Type

from yapim import Task, DependencyInput


class IdentifyProteins(Task):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.output = {
            "proteins": self.wdir.joinpath(self.record_id + ".faa"),
        }

    @staticmethod
    def requires() -> List[Union[str, Type]]:
        return []

    @staticmethod
    def depends() -> List[DependencyInput]:
        pass

    def run(self):
        self.single(
            self.program[
                "-i", self.input["fasta"],
                "-a", self.output["proteins"],
                (*self.added_flags)
            ]
        )
```

After calling its superclass initializer, we can define the expected output that this task will generate.

**Any data that is of `str` or `Path` type will be validated for existence after the `Task` completes its `run()` method**.

Here, we have listed that this task will output a file in its working directory, and this will be checked when the `Task` completes. Finally, as a matter of convenience, we replaced the hard-coded output value in the `run()` method with its respective value in `self.output`.

### Where did `self.input["fasta"]` come from??

When launched, YAPIM begins by populating the input that will be used to launch this pipeline. Advanced users may see the `InputLoader` section of the documentation.

For our tutorial, note that all input loaded from running YAPIM via its CLI will populate into the following categories based on the file's extension

```
fasta: .fna, .faa, .fasta, .fa
fastq_1: _1.fastq, _1.fq, .1.fastq, .1.fq
fastq_2: _2.fastq, _2.fq, .2.fastq, .2.fq
gff3: .gff3, .gff
```

So, in the case of an input directory containing the following files:

```
-- directory
   |-- genome1.fna
   |-- genome1.gff3
   |-- SRR1234.1.fq
   |-- SRR1234.2.fq
```

would populate into the following input data:

```python
{
    "genome1": {
        "fasta": "genome1.fna",
        "gff3": "genome1.gff3"
    },
    "SRR1234": {
        "fastq_1": "SRR1234.1.fq",
        "fastq_2": "SRR1234.2.fq"
    }
}
```

This implementation allows any class that inherits from `Task` to operate on each individual key:value pair, whereas classes that inherit from `AggregateTask` can operate on the entire input set.

## Testing step 1

With our first step written, it may be useful to test it and ensure that it completes as expected. We should also use this opportunity to generate a config file.

From the command line, we can validate our code and also generate a config file using the command `yapim create`. From within the `demo` directory, run:

```shell
yapim create -t tasks
```

A new directory named `tasks-pipeline` will generate containing the file `tasks-config.yaml`:

```yaml
---  # document start

###########################################
## Global settings
GLOBAL:
  # Maximum threads/cpus to use in analysis
  MaxThreads: 10
  # Maximum memory to use (in GB)
  MaxMemory: 100

###########################################
## SLURM run settings
SLURM:
  ## Set to True if using SLURM
  USE_CLUSTER: false
  ## Pass any flags you wish below
  ## DO NOT PASS the following:
  ## --nodes, --ntasks, --mem, --cpus-per-task
  --qos: unlim
  --job-name: EukMS
  user-id: uid

###########################################
## Pipeline input section
INPUT:
  root: all

###########################################

IdentifyProteins:
  # Number of threads task will use
  threads: 1
  # Amount of memory task will use (in GB)
  memory: 8
  time: "4:00:00"
...  # document end
```

This file represents the primary interface for users to modify calling parameters to your pipeline. For example, in the `GLOBAL` section, a user can set the maximum allowable threads and memory that this pipeline can use. Users may also set their `SLURM` user and partition settings, and can adjust the input to this pipeline.

Let's fill in the section related to the class we just wrote:

```yaml
...
IdentifyProteins:
  # Number of threads task will use
  threads: 1
  # Amount of memory task will use (in GB)
  memory: 8
  time: "4:00:00"
  program: prodigal
  FLAGS:
    -p single
...
```

Here, we define the item `program` (which we had used in our implementation above), and we create a field to allow users to pass flags to this program.

------

## Step 2: Perform quality analysis on each assembly

------

## Step 3: Annotate assemblies that passed quality filter
